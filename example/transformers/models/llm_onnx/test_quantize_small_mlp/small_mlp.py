import torch
import torch.nn as nn
import torch.nn.functional as F
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType


class SmallMlp(nn.Module):

    def __init__(self):
        super(SmallMlp, self).__init__()
        self.fc1 = nn.Linear(800, 120)
        self.fc2 = nn.Linear(120, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


if __name__ == "__main__":
    torch_model = SmallMlp()
    torch_input = torch.randn(800)
    onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)
    model_name = "small_mlp.onnx"
    onnx_program.save(model_name)
    # For some reason, the MLP needs to be preprocessed before quantization to get have the model  
    # actually show quantization in the graph. Only running quantization on the model seems to 
    # do nothing. Test case: 
    # model_preprocessed = "small_mlp.onnx"
    # model_quant = "small_mlp_quantized.onnx"
    # # weight_type option works, but activation_type doesn't work even though their documentation shows it
    # # as an option: https://lenisha.github.io/onnxruntime/docs/how-to/quantization.html#:~:text=Quantizing%20an%20ONNX%20model%20.%20There%20are%203%20ways%20of
    # quantized_model = quantize_dynamic(model_preprocessed, model_quant, weight_type=QuantType.QUInt8)

    # Preprocessed model is generated by running:
    # python -m onnxruntime.quantization.preprocess --input small_mlp.onnx --output small_mlp_infer.onnx
    model_preprocessed = "small_mlp_infer.onnx"
    model_quant = "small_mlp_quantized.onnx"
    # weight_type option works, but activation_type doesn't work even though their documentation shows it
    # as an option: https://lenisha.github.io/onnxruntime/docs/how-to/quantization.html#:~:text=Quantizing%20an%20ONNX%20model%20.%20There%20are%203%20ways%20of
    quantized_model = quantize_dynamic(model_preprocessed, model_quant, weight_type=QuantType.QUInt8)